{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"文本 使用RNN生成文本.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNaZkt7Chx6ap+TqbLiTMFu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qnmDJhhWQDV-","colab_type":"text"},"source":["##循环神经网络（RNN）文本生成\n","\n","本教程演示如何使用基于字符的 RNN 生成文本。我们将使用 Andrej Karpathy 在《循环神经网络不合理的有效性》一文中提供的莎士比亚作品数据集。给定此数据中的一个字符序列 （“Shakespear”），训练一个模型以预测该序列的下一个字符（“e”）。通过重复调用该模型，可以生成更长的文本序列。\n","\n","> 请注意：启用 GPU 加速可以更快地执行此笔记本。在 Colab 中依次选择：运行时 > 更改运行时类型 > 硬件加速器 > GPU。如果在本地运行，请确保 TensorFlow 的版本为 1.11 或更高。\n","\n","本教程包含使用 tf.keras 和 eager execution 实现的可运行代码。以下是当本教程中的模型训练 30 个周期 （epoch），并以字符串 “Q” 开头时的示例输出：\n","```\n","\n","```\n","虽然有些句子符合语法规则，但是大多数句子没有意义。这个模型尚未学习到单词的含义，但请考虑以下几点：\n","\n","* 此模型是基于字符的。训练开始时，模型不知道如何拼写一个英文单词，甚至不知道单词是文本的一个单位。\n","\n","* 输出文本的结构类似于剧本 -- 文本块通常以讲话者的名字开始；而且与数据集类似，讲话者的名字采用全大写字母。\n","\n","* 如下文所示，此模型由小批次 （batch） 文本训练而成（每批 100 个字符）。即便如此，此模型仍然能生成更长的文本序列，并且结构连贯。\n","\n","##设置\n","导入 TensorFlow 和其他库"]},{"cell_type":"code","metadata":{"id":"ooS4Nw_OQ742","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":38},"outputId":"e8eb4d6b-1aec-45e7-c43a-5b9cfa66579a","executionInfo":{"status":"ok","timestamp":1580560961446,"user_tz":-480,"elapsed":1567,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["try:\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SuH39zhgREKF","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import,division,print_function,unicode_literals\n","\n","import tensorflow as tf\n","\n","import numpy as np\n","import os\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7k_TQ3qsRX15","colab_type":"text"},"source":["###下载莎士比亚数据集\n","修改下面一行代码，在你自己的数据上运行此代码。"]},{"cell_type":"code","metadata":{"id":"tfd5nQzgRf9T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":79},"outputId":"d2b93e6f-58f6-44db-9e9f-73cfce3e1eb9","executionInfo":{"status":"ok","timestamp":1580560969951,"user_tz":-480,"elapsed":10053,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1122304/1115394 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QaU9_QAPRs9k","colab_type":"text"},"source":["###读取数据\n","首先，看一看文本："]},{"cell_type":"code","metadata":{"id":"-XkfROSLRxQM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":38},"outputId":"657ae4cc-7068-4aa9-865c-547a645c307c","executionInfo":{"status":"ok","timestamp":1580560969952,"user_tz":-480,"elapsed":10044,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["text = open(path_to_file,'rb').read().decode(encoding='utf-8')\n","print('Length of text:{} characters'.format(len(text)))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Length of text:1115394 characters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RS04tvTWSH_d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":332},"outputId":"60818646-4c2c-4945-f514-8ddb45fdf8f9","executionInfo":{"status":"ok","timestamp":1580560969953,"user_tz":-480,"elapsed":10036,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["#看一看文本中的前250个字符\n","print(text[:250])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IluATk6MSTr6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":38},"outputId":"a6369e08-3ec1-406d-f9e1-6fe7c251ecae","executionInfo":{"status":"ok","timestamp":1580560969953,"user_tz":-480,"elapsed":10027,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["#文本中的非重复字符\n","vocab = sorted(set(text))\n","print('{} unique characters'.format(len(vocab)))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["65 unique characters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OrS3IgGKSpEO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2c972211-619f-42a6-dd5c-c1403c81b5bf","executionInfo":{"status":"ok","timestamp":1580560969954,"user_tz":-480,"elapsed":10019,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["vocab"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\n',\n"," ' ',\n"," '!',\n"," '$',\n"," '&',\n"," \"'\",\n"," ',',\n"," '-',\n"," '.',\n"," '3',\n"," ':',\n"," ';',\n"," '?',\n"," 'A',\n"," 'B',\n"," 'C',\n"," 'D',\n"," 'E',\n"," 'F',\n"," 'G',\n"," 'H',\n"," 'I',\n"," 'J',\n"," 'K',\n"," 'L',\n"," 'M',\n"," 'N',\n"," 'O',\n"," 'P',\n"," 'Q',\n"," 'R',\n"," 'S',\n"," 'T',\n"," 'U',\n"," 'V',\n"," 'W',\n"," 'X',\n"," 'Y',\n"," 'Z',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'v',\n"," 'w',\n"," 'x',\n"," 'y',\n"," 'z']"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"mTVWMpCcSueX","colab_type":"text"},"source":["###处理文本\n","**向量化文本**\n","\n","在训练之前，我们需要将字符串映射到数字表示值。创建两个查找表格：一个将字符映射到数字，另一个将数字映射到字符。"]},{"cell_type":"code","metadata":{"id":"0FLCDGKsS8wf","colab_type":"code","colab":{}},"source":["#创建非重复字符到索引的映射\n","char2idx = {u:i for i,u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","text_as_int = np.array([char2idx[c] for c in text])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0qxH7ohqTlcc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":38},"outputId":"95136ba3-2b05-4e85-9f2f-f27a089981de","executionInfo":{"status":"ok","timestamp":1580560969956,"user_tz":-480,"elapsed":10000,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["text_as_int"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([18, 47, 56, ..., 45,  8,  0])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"rQcD6aQbTnam","colab_type":"text"},"source":["现在，每个字符都有一个整数表示值。请注意，我们将字符映射至索引 0 至 len(unique)."]},{"cell_type":"code","metadata":{"id":"mJ0FO_wyTt9d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":500},"outputId":"cfdb5e25-14f8-417c-8a94-9177176b5873","executionInfo":{"status":"ok","timestamp":1580560969957,"user_tz":-480,"elapsed":9988,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["print('{')\n","for char,_ in zip(char2idx,range(20)):\n","  print(' {:4s}: {:3d},'.format(repr(char),char2idx[char]))\n","\n","print('  ...\\n')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["{\n"," '\\n':   0,\n"," ' ' :   1,\n"," '!' :   2,\n"," '$' :   3,\n"," '&' :   4,\n"," \"'\" :   5,\n"," ',' :   6,\n"," '-' :   7,\n"," '.' :   8,\n"," '3' :   9,\n"," ':' :  10,\n"," ';' :  11,\n"," '?' :  12,\n"," 'A' :  13,\n"," 'B' :  14,\n"," 'C' :  15,\n"," 'D' :  16,\n"," 'E' :  17,\n"," 'F' :  18,\n"," 'G' :  19,\n","  ...\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0A_R8cUhULa7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":58},"outputId":"68e7dc7c-96d9-4c7b-c999-f69a33aa41d5","executionInfo":{"status":"ok","timestamp":1580560969957,"user_tz":-480,"elapsed":9976,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["# 显示文本首 13 个字符的整数映射\n","print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]),text_as_int[:13]))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O6hWEuolU2Iv","colab_type":"text"},"source":["###预测任务\n","给定一个字符或者一个字符序列，下一个最可能出现的字符是什么？这就是我们训练模型要执行的任务。输入进模型的是一个字符序列，我们训练这个模型来预测输出 -- 每个时间步（time step）预测下一个字符是什么。\n","\n","由于 RNN 是根据前面看到的元素维持内部状态，那么，给定此时计算出的所有字符，下一个字符是什么？\n","\n","###创建训练样本和目标\n","接下来，将文本划分为样本序列。每个输入序列包含文本中的 `seq_length` 个字符。\n","\n","对于每个输入序列，其对应的目标包含相同长度的文本，但是向右顺移一个字符。\n","\n","将文本拆分为长度为 `seq_length+1` 的文本块。例如，假设 `seq_length` 为 4 而且文本为 “Hello”， 那么输入序列将为 “Hell”，目标序列将为 “ello”。\n","\n","为此，首先使用 tf.data.Dataset.from_tensor_slices 函数把文本向量转换为字符索引流。"]},{"cell_type":"code","metadata":{"id":"CH6a6AMYx1ST","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"edb6c594-f3e7-4052-f123-ce7619ed18f7","executionInfo":{"status":"ok","timestamp":1580561248734,"user_tz":-480,"elapsed":1218,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["# 设定每个输入句子长度的最大值\n","seq_length = 100\n","examples_per_epoch = len(text)//seq_length\n","\n","#创建训练样本/目标\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","for i in char_dataset.take(5):\n","  print(idx2char[i.numpy()])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["F\n","i\n","r\n","s\n","t\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7pyWsyfNyqzN","colab_type":"text"},"source":["batch 方法使我们能轻松把单个字符转换为所需长度的序列。"]},{"cell_type":"code","metadata":{"id":"aFPyI3Puy0UH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":142},"outputId":"9b3cc248-9b03-425e-9e02-5eecb6453002","executionInfo":{"status":"ok","timestamp":1580561380064,"user_tz":-480,"elapsed":1341,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["sequences = char_dataset.batch(seq_length+1,drop_remainder=True)\n","\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vMBuUQ3ezK0k","colab_type":"text"},"source":["对于每个序列，使用 map 方法先复制再顺移，以创建输入文本和目标文本。map 方法可以将一个简单的函数应用到每一个批次 （batch）。"]},{"cell_type":"code","metadata":{"id":"bWRhZilV0FDf","colab_type":"code","colab":{}},"source":["def split_input_target(chunk):\n","  input_text = chunk[:-1]\n","  target_text = chunk[1:]\n","  return input_text,target_text\n","\n","dataset = sequences.map(split_input_target)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4sW2Ret-0jXQ","colab_type":"text"},"source":["打印第一批样本的输入与目标值："]},{"cell_type":"code","metadata":{"id":"bV8e5ee90nqM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":79},"outputId":"eae41de8-01ef-4f95-a9ac-a909ae785c44","executionInfo":{"status":"ok","timestamp":1580561917711,"user_tz":-480,"elapsed":856,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["for input_example,target_example in dataset.take(1):\n","  print('输入数据：',repr(''.join(idx2char[input_example.numpy()])))\n","  print('输出数据：',repr(''.join(idx2char[target_example.numpy()])))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["输入数据： 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","输出数据： 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xaIwRAmI1ONl","colab_type":"text"},"source":["这些向量的每个索引均作为一个时间步来处理。作为时间步 0 的输入，模型接收到 “F” 的索引，并尝试预测 “i” 的索引为下一个字符。在下一个时间步，模型执行相同的操作，但是 RNN 不仅考虑当前的输入字符，还会考虑上一步的信息。"]},{"cell_type":"code","metadata":{"id":"WcZiGO6u1ZwP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":332},"outputId":"8091eb10-61b2-498d-8422-061015c9c5c8","executionInfo":{"status":"ok","timestamp":1580562370483,"user_tz":-480,"elapsed":1179,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["for i,(input_idx,target_idx) in enumerate(zip(input_example[:5],target_example[:5])):\n","  print(\"Step {:4d}\".format(i))\n","  print(\" input: {} ({:s})\".format(input_idx,repr(idx2char[input_idx])))\n","  print(\" expected output: {} ({:s})\".format(target_idx,repr(idx2char[target_idx])))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Step    0\n"," input: 18 ('F')\n"," expected output: 47 ('i')\n","Step    1\n"," input: 47 ('i')\n"," expected output: 56 ('r')\n","Step    2\n"," input: 56 ('r')\n"," expected output: 57 ('s')\n","Step    3\n"," input: 57 ('s')\n"," expected output: 58 ('t')\n","Step    4\n"," input: 58 ('t')\n"," expected output: 1 (' ')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1pLsORvx28qe","colab_type":"text"},"source":["###创建训练批次\n","前面我们使用 tf.data 将文本拆分为可管理的序列。但是在把这些数据输送至模型之前，我们需要将数据重新排列 （shuffle） 并打包为批次。"]},{"cell_type":"code","metadata":{"id":"uUp5jsa14EId","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":58},"outputId":"d4f20558-2d89-4a87-af77-8cb80ae80b0c","executionInfo":{"status":"ok","timestamp":1580562794185,"user_tz":-480,"elapsed":796,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["#批大小\n","BATCH_SIZE = 64\n","\n","# 设定缓冲区大小，以重新排列数据集\n","# （TF 数据被设计为可以处理可能是无限的序列，\n","# 所以它不会试图在内存中重新排列整个序列。相反，\n","# 它维持一个缓冲区，在缓冲区重新排列元素。） \n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n","dataset"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"WA-j8uit4kNM","colab_type":"text"},"source":["###创建模型\n","使用 `tf.keras.Sequential` 定义模型。在这个简单的例子中，我们使用了三个层来定义模型：\n","\n","* `tf.keras.layers.Embedding`：输入层。一个可训练的对照表，它会将每个字符的数字映射到一个 `embedding_dim` 维度的向量。\n","* `tf.keras.layers.GRU`：一种 RNN 类型，其大小由 `units=rnn_units` 指定（这里你也可以使用一个 LSTM 层）。\n","* `tf.keras.layers.Dense`：输出层，带有 `vocab_size` 个输出。"]},{"cell_type":"code","metadata":{"id":"GGp7pbx_5HF0","colab_type":"code","colab":{}},"source":["# 词集的长度\n","vocab_size = len(vocab)\n","\n","# 嵌入的维度\n","embedding_dim = 256\n","\n","# RNN 的单元数量\n","rnn_units = 1024"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NndQMZG5MbD","colab_type":"code","colab":{}},"source":["def build_model(vocab_size,embedding_dim,rnn_units,batch_size):\n","  model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size,embedding_dim,\n","                          batch_input_shape=[batch_size,None]),\n","    tf.keras.layers.GRU(rnn_units,\n","                        return_sequences=True,\n","                        stateful=True,\n","                        recurrent_initializer='glorot_uniform'),\n","    tf.keras.layers.Dense(vocab_size)\n","  ])\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ScQK4GNY6QBX","colab_type":"code","colab":{}},"source":["model = build_model(\n","    vocab_size=len(vocab),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units,\n","    batch_size=BATCH_SIZE\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I5st8e376ml6","colab_type":"text"},"source":["对于每个字符，模型会查找嵌入，把嵌入当作输入运行 GRU 一个时间步，并用密集层生成逻辑回归 （logits），预测下一个字符的对数可能性。\n","![](https://github.com/littlebeanbean7/docs/blob/master/site/en/tutorials/text/images/text_generation_training.png?raw=1)\n","###试试这个模型\n","现在运行这个模型，看看它是否按预期运行。\n","\n","首先检查输出的形状："]},{"cell_type":"code","metadata":{"id":"J9f-vkOe7Dev","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":38},"outputId":"740d14b4-9be4-4e13-ba84-3c7b9840c6d6","executionInfo":{"status":"ok","timestamp":1580563603850,"user_tz":-480,"elapsed":9571,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["for input_example_batch,target_example_batch in dataset.take(1):\n","  example_batch_predictions = model(input_example_batch)\n","  print(example_batch_predictions.shape,\"# (batch_size,sequence_length,vocab_size)\")"],"execution_count":23,"outputs":[{"output_type":"stream","text":["(64, 100, 65) # (batch_size,sequence_length,vocab_size)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O-JkGTmg7nu4","colab_type":"text"},"source":["在上面的例子中，输入的序列长度为 100， 但是这个模型可以在任何长度的输入上运行："]},{"cell_type":"code","metadata":{"id":"7OxNECDO7yRr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":331},"outputId":"286a3e7c-2b81-468c-fbf0-8e20f091e22c","executionInfo":{"status":"ok","timestamp":1580563646312,"user_tz":-480,"elapsed":1080,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["model.summary()"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (64, None, 256)           16640     \n","_________________________________________________________________\n","gru (GRU)                    (64, None, 1024)          3938304   \n","_________________________________________________________________\n","dense (Dense)                (64, None, 65)            66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6j3PKVmi70Lq","colab_type":"text"},"source":["为了获得模型的实际预测，我们需要从输出分布中抽样，以获得实际的字符索引。这个分布是根据对字符集的逻辑回归定义的。\n","\n","请注意：从这个分布中 *抽样* 很重要，因为取分布的 最大值自变量点集（argmax） 很容易使模型卡在循环中。\n","\n","试试这个批次中的第一个样本："]},{"cell_type":"code","metadata":{"id":"VCAP4nCP8CrC","colab_type":"code","colab":{}},"source":["sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oAPGjwGX8oNN","colab_type":"text"},"source":["这使我们得到每个时间步预测的下一个字符的索引。"]},{"cell_type":"code","metadata":{"id":"w0X-s2XB8tAM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":163},"outputId":"56f0a8e1-ce66-424e-98cb-504ef1d7aad2","executionInfo":{"status":"ok","timestamp":1580563891126,"user_tz":-480,"elapsed":1297,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["sampled_indices"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([20, 17, 39, 63, 32, 30, 42, 36, 34, 16,  7,  6, 42, 19, 35, 46,  8,\n","       26, 41,  1, 63, 17, 21, 12, 52, 51, 49, 45, 38,  9, 35, 49, 55, 53,\n","       23,  0, 16, 25,  6, 23, 16, 40, 54, 32, 48, 59, 41, 11, 30, 32, 22,\n","       45,  9, 17, 60, 32, 46, 49, 55, 64, 36, 22,  8, 12,  9, 34, 58, 30,\n","        6, 44, 29, 36, 54, 48, 33, 27, 26, 61,  4,  0, 28, 16, 35, 55, 50,\n","       53,  9, 54, 35, 15, 24, 31, 29, 32,  9, 15, 26, 37, 59, 48])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"AoTxxUYu8v5G","colab_type":"text"},"source":["解码它们，以查看此未经训练的模型预测的文本："]},{"cell_type":"code","metadata":{"id":"b1iT6yzl81Kg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":142},"outputId":"a6c371e5-1c76-4a65-f37b-69b24b9a2993","executionInfo":{"status":"ok","timestamp":1580564040026,"user_tz":-480,"elapsed":1097,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["print(\"Input: \\n\",repr(\"\".join(idx2char[input_example_batch[0]])))\n","print()\n","print(\"Next Char Predictions: \\n\",repr(\"\".join(idx2char[sampled_indices])))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Input: \n"," \"me like men:\\n'When griping grief the heart doth wound,\\nAnd doleful dumps the mind oppress,\\nThen musi\"\n","\n","Next Char Predictions: \n"," 'HEayTRdXVD-,dGWh.Nc yEI?nmkgZ3WkqoK\\nDM,KDbpTjuc;RTJg3EvThkqzXJ.?3VtR,fQXpjUONw&\\nPDWqlo3pWCLSQT3CNYuj'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yBYFjMQd9UTB","colab_type":"text"},"source":["##训练模型\n","此时，这个问题可以被视为一个标准的分类问题：给定先前的 RNN 状态和这一时间步的输入，预测下一个字符的类别。\n","\n","###添加优化器和损失函数\n","标准的 `tf.keras.losses.sparse_categorical_crossentropy` 损失函数在这里适用，因为它被应用于预测的最后一个维度。\n","\n","因为我们的模型返回逻辑回归，所以我们需要设定命令行参数 `from_logits`。"]},{"cell_type":"code","metadata":{"id":"5G4Ob983912c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":79},"outputId":"f23839f5-a498-42e8-99d4-168893e5d0c6","executionInfo":{"status":"ok","timestamp":1580564470337,"user_tz":-480,"elapsed":911,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["def loss(labels,logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)\n","\n","example_batch_loss = loss(target_example_batch,example_batch_predictions)\n","print(\"Prediction shape: \",example_batch_predictions.shape,\" # (batch_size,sequence_length,vocab_size)\")\n","print(\"scalar_loss:      \",example_batch_loss.numpy().mean())"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Prediction shape:  (64, 100, 65)  # (batch_size,sequence_length,vocab_size)\n","scalar_loss:       4.1739306\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B2Q0cnHr-9Yx","colab_type":"text"},"source":["使用 `tf.keras.Model.compile` 方法配置训练步骤。我们将使用 `tf.keras.optimizers.Adam` 并采用默认参数，以及损失函数。"]},{"cell_type":"code","metadata":{"id":"mg1VIvda_Kt8","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam',loss=loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BGnEO6ik_Quj","colab_type":"text"},"source":["###配置检查点\n","使用 `tf.keras.callbacks.ModelCheckpoint `来确保训练过程中保存检查点。"]},{"cell_type":"code","metadata":{"id":"s8fA5uWb_cIh","colab_type":"code","colab":{}},"source":["# 检查点保存至的目录\n","checkpoint_dir = './training_checkpoints'\n","\n","# 检查点的文件名\n","checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt_{epoch}')\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b_GfrjJHADG5","colab_type":"text"},"source":["###执行训练\n","为保持训练时间合理，使用 10 个周期来训练模型。在 Colab 中，将运行时设置为 GPU 以加速训练。"]},{"cell_type":"code","metadata":{"id":"cOC-htI_AIUj","colab_type":"code","colab":{}},"source":["EPOCHS = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgoV7QgTAK3N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":478},"outputId":"2f498cce-0e81-4f7b-d440-4704acf2d7b9","executionInfo":{"status":"ok","timestamp":1580564947756,"user_tz":-480,"elapsed":116087,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["history = model.fit(dataset,epochs=EPOCHS,callbacks=[checkpoint_callback])"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Train for 172 steps\n","Epoch 1/10\n","172/172 [==============================] - 12s 69ms/step - loss: 2.6830\n","Epoch 2/10\n","172/172 [==============================] - 11s 64ms/step - loss: 1.9568\n","Epoch 3/10\n","172/172 [==============================] - 11s 63ms/step - loss: 1.6895\n","Epoch 4/10\n","172/172 [==============================] - 11s 65ms/step - loss: 1.5415\n","Epoch 5/10\n","172/172 [==============================] - 11s 66ms/step - loss: 1.4536\n","Epoch 6/10\n","172/172 [==============================] - 11s 66ms/step - loss: 1.3938\n","Epoch 7/10\n","172/172 [==============================] - 12s 67ms/step - loss: 1.3482\n","Epoch 8/10\n","172/172 [==============================] - 11s 67ms/step - loss: 1.3107\n","Epoch 9/10\n","172/172 [==============================] - 11s 66ms/step - loss: 1.2755\n","Epoch 10/10\n","172/172 [==============================] - 12s 69ms/step - loss: 1.2432\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C0ZZzwZ2AV04","colab_type":"text"},"source":["###生成文本\n","恢复最新的检查点\n","为保持此次预测步骤简单，将批大小设定为 1。\n","\n","由于 RNN 状态从时间步传递到时间步的方式，模型建立好之后只接受固定的批大小。\n","\n","若要使用不同的 batch_size 来运行模型，我们需要重建模型并从检查点中恢复权重。"]},{"cell_type":"code","metadata":{"id":"jwwyczZPAzby","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":38},"outputId":"e97458cb-1de3-44b9-e5f2-d5a767de5bef","executionInfo":{"status":"ok","timestamp":1580564987090,"user_tz":-480,"elapsed":2555,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["tf.train.latest_checkpoint(checkpoint_dir)"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'./training_checkpoints/ckpt_10'"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"Yi5oiS7yA7KI","colab_type":"code","colab":{}},"source":["model = build_model(vocab_size,embedding_dim,rnn_units,batch_size=1)\n","\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","\n","model.build(tf.TensorShape([1,None]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2JhV_K6BcEw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":331},"outputId":"ce8af1af-c6dd-4677-9ea4-bbfb8cf9aea5","executionInfo":{"status":"ok","timestamp":1580565132117,"user_tz":-480,"elapsed":2232,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["model.summary()"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (1, None, 256)            16640     \n","_________________________________________________________________\n","gru_1 (GRU)                  (1, None, 1024)           3938304   \n","_________________________________________________________________\n","dense_1 (Dense)              (1, None, 65)             66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"40M4PimFBeW4","colab_type":"text"},"source":["###预测循环\n","下面的代码块生成文本：\n","\n","* 首先设置起始字符串，初始化 RNN 状态并设置要生成的字符个数。\n","\n","* 用起始字符串和 RNN 状态，获取下一个字符的预测分布。\n","\n","* 然后，用分类分布计算预测字符的索引。把这个预测字符当作模型的下一个输入。\n","\n","* 模型返回的 RNN 状态被输送回模型。现在，模型有更多上下文可以学习，而非只有一个字符。在预测出下一个字符后，更改过的 RNN 状态被再次输送回模型。模型就是这样，通过不断从前面预测的字符获得更多上下文，进行学习。\n","![替代文字](https://github.com/littlebeanbean7/docs/blob/master/site/en/tutorials/text/images/text_generation_sampling.png?raw=1)\n","为生成文本，模型的输出被输送回模型作为输入\n","\n","查看生成的文本，你会发现这个模型知道什么时候使用大写字母，什么时候分段，而且模仿出了莎士比亚式的词汇。由于训练的周期小，模型尚未学会生成连贯的句子。"]},{"cell_type":"code","metadata":{"id":"eEGTrIa8CA0m","colab_type":"code","colab":{}},"source":["def generate_text(model,start_string):\n","  # 评估步骤（用学习过的模型生成文本）\n","\n","  # 要生成的字符个数\n","  num_generate = 1000\n","\n","  # 将起始字符串转换为数字（向量化）\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval,0)\n","\n","  #空字符串有于存储结果\n","  text_generated = []\n","\n","  # 低温度会生成更可预测的文本\n","  # 较高温度会生成更令人惊讶的文本\n","  # 可以通过试验以找到最好的设定\n","  temperature = 1.0\n","\n","  # 这里批大小为1\n","  model.reset_states()\n","  for i in range(num_generate):\n","    predictions = model(input_eval)\n","    # 删除批次的维度\n","    predictions = tf.squeeze(predictions,0)\n","\n","    # 用分类分布预测模型返回的字符\n","    predictions = predictions / temperature\n","    predicted_id = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n","\n","    # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入\n","    input_eval = tf.expand_dims([predicted_id],0)\n","\n","    text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_2R1HCJEKnv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":941},"outputId":"2ec8f7f0-e2ff-4277-dd7f-335e4c8f208d","executionInfo":{"status":"ok","timestamp":1580565870450,"user_tz":-480,"elapsed":4806,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["print(generate_text(model, start_string=u\"ROMEO: \"))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["ROMEO: what yet is good wind of Gluecy,\n","Where the kingmants conspires stones to go.\n","\n","First Lord:\n","To the queen's son!\n","I will spend it.\n","\n","KING RICHARD II:\n","To the lieg lives between herself,\n","Siting to the sun a hole-tyrng.\n","\n","ISABELLA:\n","\n","ISABELLA:\n","O my proportion, you know, thou! we hanged\n","Doth the absent' hands of unthr outward sens.\n","So thou should see what that ve quickly sound:\n","A thousand fast his womb, shall kindred him to\n","ex, young masters, Varrian watch,\n","Which words menot make e fearful nevern,\n","That ballad is runis head, than we pardon'd in Padua.\n","\n","RATCLIFF:\n","My lord,\n","It is now told for CLIUST:\n","All made honour with me.\n","\n","HENRY BOLINGBROKE:\n","What will I sex't\n","me to instruct froan.\n","\n","BUCKINGHAM:\n","With all this island my hock'd mercy?\n","\n","And RICHARD II:\n","One thought into come from suspacradity,\n","Thy beauteen waves full oath for your cheeks\n","Perform'd, rather part to thanks.\n","\n","HENRY II:\n","Too y sea\n","With wine of mine; I have revenged of you:\n","I wake a darce of the point and dark seft\n","Thy conclock it fitth thy bl\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gJQY8Yh9ESRT","colab_type":"text"},"source":["若想改进结果，最简单的方式是延长训练时间 （试试 `EPOCHS=30`）。\n","\n","你还可以试验使用不同的起始字符串，或者尝试增加另一个 RNN 层以提高模型的准确率，亦或调整温度参数以生成更多或者更少的随机预测。\n","\n","###高级：自定义训练\n","上面的训练步骤简单，但是能控制的地方不多。\n","\n","至此，你已经知道如何手动运行模型。现在，让我们打开训练循环，并自己实现它。这是一些任务的起点，例如实现 课程学习 以帮助稳定模型的开环输出。\n","\n","你将使用 `tf.GradientTape` 跟踪梯度。关于此方法的更多信息请参阅 **eager execution** 指南。\n","\n","步骤如下：\n","\n","* 首先，初始化 RNN 状态，使用 `tf.keras.Model.reset_states` 方法。\n","\n","* 然后，迭代数据集（逐批次）并计算每次迭代对应的 *预测*。\n","\n","* 打开一个 `tf.GradientTape` 并计算该上下文时的预测和损失。\n","\n","* 使用 `tf.GradientTape.grads` 方法，计算当前模型变量情况下的损失梯度。\n","\n","* 最后，使用优化器的 `tf.train.Optimizer.apply_gradients` 方法向下迈出一步。"]},{"cell_type":"code","metadata":{"id":"mlSnPV5hFfnt","colab_type":"code","colab":{}},"source":["model = build_model(\n","    vocab_size=len(vocab),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units,\n","    batch_size=BATCH_SIZE\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nr7yQ8a6F0ls","colab_type":"code","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4O-KozFlF6iL","colab_type":"code","colab":{}},"source":["@tf.function\n","def train_step(inp,target):\n","  with tf.GradientTape() as tape:\n","    predictions = model(inp)\n","    loss = tf.reduce_mean(\n","        tf.keras.losses.sparse_categorical_crossentropy(\n","            target,predictions,from_logits=True\n","        )\n","    )\n","  grads = tape.gradient(loss,model.trainable_variables)\n","  optimizer.apply_gradients(zip(grads,model.trainable_variables))\n","\n","  return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zmM16gifG_tV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"96059907-0d7e-45cd-a682-0d489012de96","executionInfo":{"status":"ok","timestamp":1580567125026,"user_tz":-480,"elapsed":104630,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["# 训练步骤\n","EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  # 在每个训练周期开始时，初始化隐藏状态\n","  # 隐藏状态最初为 None\n","  hidden = model.reset_states()\n","\n","  for (batch_n, (inp,target)) in enumerate(dataset):\n","    loss = train_step(inp,target)\n","\n","    if batch_n % 100 ==0:\n","      template = 'Epoch {} Batch {}  Loss {}'\n","      print(template.format(epoch+1,batch_n,loss))\n","\n","  # 每 5 个训练周期，保存（检查点）1 次模型\n","  if (epoch + 1 )% 5==0:\n","    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch+1,loss))\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","\n","model.save_weights(checkpoint_prefix.format(epoch=epoch))"],"execution_count":41,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n","WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n","Epoch 1 Batch 0  Loss 4.173054218292236\n","Epoch 1 Batch 100  Loss 2.369464159011841\n","Epoch 1 Loss 2.1767\n","Time taken for 1 epoch 11.035712003707886 sec\n","\n","Epoch 2 Batch 0  Loss 2.1571412086486816\n","Epoch 2 Batch 100  Loss 1.956691026687622\n","Epoch 2 Loss 1.8227\n","Time taken for 1 epoch 9.936342477798462 sec\n","\n","Epoch 3 Batch 0  Loss 1.7542097568511963\n","Epoch 3 Batch 100  Loss 1.6618119478225708\n","Epoch 3 Loss 1.6032\n","Time taken for 1 epoch 9.930558204650879 sec\n","\n","Epoch 4 Batch 0  Loss 1.5862512588500977\n","Epoch 4 Batch 100  Loss 1.5161052942276\n","Epoch 4 Loss 1.4686\n","Time taken for 1 epoch 10.111042022705078 sec\n","\n","Epoch 5 Batch 0  Loss 1.4687869548797607\n","Epoch 5 Batch 100  Loss 1.4633262157440186\n","Epoch 5 Loss 1.4740\n","Time taken for 1 epoch 10.198846817016602 sec\n","\n","Epoch 6 Batch 0  Loss 1.4143002033233643\n","Epoch 6 Batch 100  Loss 1.3810276985168457\n","Epoch 6 Loss 1.3660\n","Time taken for 1 epoch 10.302242994308472 sec\n","\n","Epoch 7 Batch 0  Loss 1.3266546726226807\n","Epoch 7 Batch 100  Loss 1.3786239624023438\n","Epoch 7 Loss 1.3006\n","Time taken for 1 epoch 10.535382747650146 sec\n","\n","Epoch 8 Batch 0  Loss 1.2928211688995361\n","Epoch 8 Batch 100  Loss 1.322239875793457\n","Epoch 8 Loss 1.2958\n","Time taken for 1 epoch 10.348063707351685 sec\n","\n","Epoch 9 Batch 0  Loss 1.24578058719635\n","Epoch 9 Batch 100  Loss 1.2551969289779663\n","Epoch 9 Loss 1.2638\n","Time taken for 1 epoch 10.370423316955566 sec\n","\n","Epoch 10 Batch 0  Loss 1.2012454271316528\n","Epoch 10 Batch 100  Loss 1.2170644998550415\n","Epoch 10 Loss 1.2705\n","Time taken for 1 epoch 10.461184978485107 sec\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4OJpzJenIsEH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}