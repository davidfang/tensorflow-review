{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"文本  基于注意力的神经机器翻译.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMdsbXnSFx9mMs18PU3XRrG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bcCoNOUP_zwO","colab_type":"text"},"source":["#基于注意力的神经机器翻译\n","\n","此笔记本训练一个将汉语翻译为英语的序列到序列（sequence to sequence，简写为 seq2seq）模型。此例子难度较高，需要对序列到序列模型的知识有一定了解。\n","\n","训练完此笔记本中的模型后，你将能够输入一个汉语句子，例如 \"你今天早晨吃的啥?\"，并返回其英语翻译 \"are you still at home?\"\n","\n","对于一个简单的例子来说，翻译质量令人满意。但是更有趣的可能是生成的注意力图：它显示在翻译过程中，输入句子的哪些部分受到了模型的注意。\n","\n","![](https://tensorflow.org/images/spanish-english.png)\n","\n","请注意：运行这个例子用一个 P100 GPU 需要花大约 10 分钟。"]},{"cell_type":"code","metadata":{"id":"uX8K9_Q4AvMM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":38},"outputId":"61981fd9-cf06-435a-f024-307f541a67ab","executionInfo":{"status":"ok","timestamp":1580615306959,"user_tz":-480,"elapsed":786,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["try:\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hb0WoeaWA3iy","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import,division,print_function,unicode_literals\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cC4OuogYBqZa","colab_type":"text"},"source":["###下载和准备数据集\n","我们将使用 http://www.manythings.org/anki/ 提供的一个语言数据集。这个数据集包含如下格式的语言翻译对："]},{"cell_type":"markdown","metadata":{"id":"yJmWCkR1By6K","colab_type":"text"},"source":["```\n","I won! 我赢了。\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"TSYOQX33CE1f","colab_type":"text"},"source":["这个数据集中有很多种语言可供选择。我们将使用英语 - 汉语数据集。为方便使用，我们在谷歌云上提供了此数据集的一份副本。但是你也可以自己下载副本。下载完数据集后，我们将采取下列步骤准备数据：\n","\n","1. 给每个句子添加一个 开始 和一个 结束 标记（token）。\n","2. 删除特殊字符以清理句子。\n","3. 创建一个单词索引和一个反向单词索引（即一个从单词映射至 id 的词典和一个从 id 映射至单词的词典）。\n","4. 将每个句子填充（pad）到最大长度。"]},{"cell_type":"code","metadata":{"id":"n2Q1zOQVDr6D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":268},"outputId":"73403b59-5a2b-4e02-f4ef-7adf968f279d","executionInfo":{"status":"ok","timestamp":1580616059250,"user_tz":-480,"elapsed":2493,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["!wget http://www.manythings.org/anki/cmn-eng.zip"],"execution_count":12,"outputs":[{"output_type":"stream","text":["--2020-02-02 04:01:00--  http://www.manythings.org/anki/cmn-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:3037::6818:6cc4, ...\n","Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 941294 (919K) [application/zip]\n","Saving to: ‘cmn-eng.zip’\n","\n","\rcmn-eng.zip           0%[                    ]       0  --.-KB/s               \rcmn-eng.zip          72%[=============>      ] 665.63K  3.25MB/s               \rcmn-eng.zip         100%[===================>] 919.23K  3.63MB/s    in 0.2s    \n","\n","2020-02-02 04:01:01 (3.63 MB/s) - ‘cmn-eng.zip’ saved [941294/941294]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4PpMlLiWD2H8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":80},"outputId":"fe9a9759-9671-4561-d955-7607e732c4ac","executionInfo":{"status":"ok","timestamp":1580616166378,"user_tz":-480,"elapsed":2370,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["%ls -l"],"execution_count":16,"outputs":[{"output_type":"stream","text":["total 924\n","-rw-r--r-- 1 root root 941294 Jan 11 14:48 cmn-eng.zip\n","drwxr-xr-x 1 root root   4096 Jan 13 16:38 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_ToVatDsFkg4","colab_type":"text"},"source":["下载有问题，先将文件手工下载，然后将它移动到缓存中去"]},{"cell_type":"code","metadata":{"id":"nCGFVtK_FbY7","colab_type":"code","colab":{}},"source":["!mv cmn-eng.zip ~/.keras/datasets/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JcHdbDc1CjnE","colab_type":"code","colab":{}},"source":["# 下载文件\n","path_to_zip = tf.keras.utils.get_file(\n","    'cmn-eng.zip',origin='http://www.manythings.org/anki/cmn-eng.zip',\n","    extract=True\n",")\n","\n","path_to_file = os.path.dirname(path_to_zip)+'/cmn-eng/cmn.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHfPtJbnHNqm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":38},"outputId":"f35710d4-a1f7-49ef-f996-b3f53ad0e145","executionInfo":{"status":"ok","timestamp":1580620690037,"user_tz":-480,"elapsed":3251,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["%ls ~/.keras/datasets/"],"execution_count":29,"outputs":[{"output_type":"stream","text":["_about.txt  cmn-eng.zip  cmn.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T77uHo-RG9Kq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":247},"outputId":"35ceb6b0-13c8-41e3-ec75-bca2d34441a2","executionInfo":{"status":"ok","timestamp":1580617068656,"user_tz":-480,"elapsed":1835,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["!head ~/.keras/datasets/cmn.txt"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Hi.\t嗨。\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #891077 (Martha)\n","Hi.\t你好。\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4857568 (musclegirlxyp)\n","Run.\t你用跑的。\tCC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #3748344 (egg0073)\n","Wait!\t等等！\tCC-BY 2.0 (France) Attribution: tatoeba.org #1744314 (belgavox) & #4970122 (wzhd)\n","Wait!\t等一下！\tCC-BY 2.0 (France) Attribution: tatoeba.org #1744314 (belgavox) & #5092613 (mirrorvan)\n","Hello!\t你好。\tCC-BY 2.0 (France) Attribution: tatoeba.org #373330 (CK) & #4857568 (musclegirlxyp)\n","I try.\t让我来。\tCC-BY 2.0 (France) Attribution: tatoeba.org #20776 (CK) & #5092185 (mirrorvan)\n","I won!\t我赢了。\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005192 (CK) & #5102367 (mirrorvan)\n","Oh no!\t不会吧。\tCC-BY 2.0 (France) Attribution: tatoeba.org #1299275 (CK) & #5092475 (mirrorvan)\n","Cheers!\t乾杯!\tCC-BY 2.0 (France) Attribution: tatoeba.org #487006 (human600) & #765577 (Martha)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hY2hHwpQDRE8","colab_type":"code","colab":{}},"source":["# 将 unicode 文件转换为 ascii\n","def unicode_to_ascii(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD',s)\n","                  if unicodedata.category(c) != 'Mn')\n","  \n","def preprocess_sentence(w):\n","  w = unicode_to_ascii(w.lower().strip())\n","\n","  # 在单词与跟在其后的标点符号之间插入一个空格\n","  # 例如： \"he is a boy.\" => \"he is a boy .\"\n","  # 参考：https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","\n","  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","  w = re.sub(r'[\" \"]+', \" \", w)\n","\n","  # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\")，将所有字符替换为空格\n","  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","\n","  w = w.rstrip().strip()\n","\n","  # 给句子加上开始和结束标记\n","  # 以便模型知道何时开始和结束预测\n","  w = '<start> ' + w + ' <end>'\n","  return w\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ji1Lv7r_JW9F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":59},"outputId":"5a665335-b597-4625-be2b-4dbb8066c1c2","executionInfo":{"status":"ok","timestamp":1580620870476,"user_tz":-480,"elapsed":1024,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["en_sentence = u\"I won!\"\n","cn_sentence = u\"我赢了。\"\n","\n","print(preprocess_sentence(en_sentence))\n","print(preprocess_sentence(cn_sentence).encode('utf-8'))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["<start> i won ! <end>\n","b'<start>  <end>'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EIAYVV5oKPlp","colab_type":"text"},"source":["##中文有总是还是返回西班牙语吧"]},{"cell_type":"code","metadata":{"id":"412hwYBAVvvu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":79},"outputId":"5bdfedc9-0df7-4038-a1f5-b6ed050494b9","executionInfo":{"status":"ok","timestamp":1580620791147,"user_tz":-480,"elapsed":759,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["# 下载文件\n","path_to_zip = tf.keras.utils.get_file(\n","    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n","    extract=True)\n","\n","path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""],"execution_count":30,"outputs":[{"output_type":"stream","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2646016/2638744 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jfKqRnb6Vzod","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":59},"outputId":"84d0b9a8-fb24-4dd5-b9cf-2380e7d824d7","executionInfo":{"status":"ok","timestamp":1580620881194,"user_tz":-480,"elapsed":829,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["en_sentence = u\"May I borrow this book?\"\n","sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n","print(preprocess_sentence(en_sentence))\n","print(preprocess_sentence(sp_sentence).encode('utf-8'))"],"execution_count":34,"outputs":[{"output_type":"stream","text":["<start> may i borrow this book ? <end>\n","b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y1vSvrMnV5Ra","colab_type":"code","colab":{}},"source":["# 1. 去除重音符号\n","# 2. 清理句子\n","# 3. 返回这样格式的单词对：[ENGLISH, SPANISH]\n","def create_dataset(path,num_examples):\n","  lines = io.open(path,encoding='UTF-8').read().strip().split('\\n')\n","\n","  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n","\n","  return zip(*word_pairs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ng5zqVGoW1w6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":79},"outputId":"8a3946ce-fb18-4486-8496-dc2db1aa3070","executionInfo":{"status":"ok","timestamp":1580621111324,"user_tz":-480,"elapsed":5523,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["en,sp = create_dataset(path_to_file,None)\n","print(en[-1])\n","print(sp[-1])"],"execution_count":36,"outputs":[{"output_type":"stream","text":["<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n","<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qh4bPLx8XAn7","colab_type":"code","colab":{}},"source":["def max_length(tensor):\n","  return max(len(t) for t in tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXfnEFcQXNTZ","colab_type":"code","colab":{}},"source":["def tokenize(lang):\n","  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","  lang_tokenizer.fit_on_texts(lang)\n","\n","  tensor = lang_tokenizer.texts_to_sequences(lang)\n","\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","  return tensor, lang_tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"05BKxYnlX23x","colab_type":"code","colab":{}},"source":["def load_dataset(path,num_examples=None):\n","  # 创建清理过的输入输出对\n","  targ_lang,inp_lang = create_dataset(path,num_examples)\n","\n","  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","\n","  return input_tensor,target_tensor,inp_lang_tokenizer,targ_lang_tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o42s1vVQZnMo","colab_type":"text"},"source":["####限制数据集的大小以加快实验速度（可选）\n","在超过 10 万个句子的完整数据集上训练需要很长时间。为了更快地训练，我们可以将数据集的大小限制为 3 万个句子（当然，翻译质量也会随着数据的减少而降低）："]},{"cell_type":"code","metadata":{"id":"WokqjxBbZwOS","colab_type":"code","colab":{}},"source":["#尝试实验不同大小的数据集\n","num_examples = 30000\n","input_tensor,target_tensor,inp_lang,targ_lang = load_dataset(path_to_file,num_examples)\n","\n","# 计算目标张量的最大长度 （max_length）\n","max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EjizyLeAaVvR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":38},"outputId":"d04108da-1d7f-444b-f643-fdcd3bcb465d","executionInfo":{"status":"ok","timestamp":1580622110779,"user_tz":-480,"elapsed":800,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["# 采用 80 - 20 的比例切分训练集和验证集\n","input_tensor_train,input_tensor_val,target_tensor_train,target_tensor_val = train_test_split(input_tensor,target_tensor,test_size=0.2)\n","\n","# 显示长度\n","print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"],"execution_count":41,"outputs":[{"output_type":"stream","text":["24000 24000 6000 6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wX6peapja1xB","colab_type":"code","colab":{}},"source":["def convert(lang, tensor):\n","  for t in tensor:\n","    if t!=0:\n","      print(\"%d ----> %s\" % (t,lang.index_word[t]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDexDE7lbKkq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":332},"outputId":"5325fe72-bb97-478c-b103-a51718350664","executionInfo":{"status":"ok","timestamp":1580622304054,"user_tz":-480,"elapsed":746,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["print(\"Input Language; index to word mapping\")\n","convert(inp_lang, input_tensor_train[0])\n","print()\n","print(\"Target Language; index to word mapping\")\n","convert(targ_lang,target_tensor_train[0])"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Input Language; index to word mapping\n","1 ----> <start>\n","143 ----> podria\n","4112 ----> matarlas\n","3 ----> .\n","2 ----> <end>\n","\n","Target Language; index to word mapping\n","1 ----> <start>\n","4 ----> i\n","169 ----> could\n","302 ----> kill\n","6 ----> you\n","3 ----> .\n","2 ----> <end>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bhI3FxFbbk_z","colab_type":"text"},"source":["###创建一个 tf.data 数据集"]},{"cell_type":"code","metadata":{"id":"zafQT4xMbruO","colab_type":"code","colab":{}},"source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index)+1\n","vocab_tar_size = len(targ_lang.word_index)+1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train,target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3SzfEuqcSLv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":38},"outputId":"416a0eb7-2b9c-4601-c649-14b31fe7776b","executionInfo":{"status":"ok","timestamp":1580622542404,"user_tz":-480,"elapsed":2108,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["example_input_batch,example_target_batch = next(iter(dataset))\n","example_input_batch.shape,example_target_batch.shape"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 16]), TensorShape([64, 11]))"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"VHoQ98-Pce2j","colab_type":"text"},"source":["###编写编码器 （encoder） 和解码器 （decoder） 模型\n","实现一个基于注意力的编码器 - 解码器模型。关于这种模型，你可以阅读 TensorFlow 的 神经机器翻译 (序列到序列) 教程 [链接文字](https://github.com/tensorflow/nmt)。本示例采用一组更新的 API。此笔记本实现了上述序列到序列教程中的 [注意力方程式](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism)。下图显示了注意力机制为每个输入单词分配一个权重，然后解码器将这个权重用于预测句子中的下一个单词。下图和公式是 [Luong 的论文](https://arxiv.org/abs/1508.04025v5)中注意力机制的一个例子。\n","\n","![attention mechanism ](https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg)\n","\n","输入经过编码器模型，编码器模型为我们提供形状为 (批大小，最大长度，隐藏层大小) 的编码器输出和形状为 (批大小，隐藏层大小) 的编码器隐藏层状态。\n","\n","下面是所实现的方程式：\n","\n","![attention equation 0 ](https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg)![attention equation 1](https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg)\n","\n","本教程的编码器采用 Bahdanau 注意力。在用简化形式编写之前，让我们先决定符号：\n","\n","* FC = 完全连接（密集）层\n","* EO = 编码器输出\n","* H = 隐藏层状态\n","* X = 解码器输入\n","\n","以及伪代码：\n","\n","* `score = FC(tanh(FC(EO) + FC(H)))`\n","* `attention weights = softmax(score, axis = 1)。` Softmax 默认被应用于最后一个轴，但是这里我们想将它应用于 第一个轴, 因为分数 （score） 的形状是 (批大小，最大长度，隐藏层大小)。最大长度 （max_length） 是我们的输入的长度。因为我们想为每个输入分配一个权重，所以 softmax 应该用在这个轴上。\n","* `context vector = sum(attention weights * EO, axis = 1)`。选择第一个轴的原因同上。\n","* `embedding output` = 解码器输入 X 通过一个嵌入层。\n","* `merged vector = concat(embedding output, context vector)`\n","* 此合并后的向量随后被传送到 GRU\n","每个步骤中所有向量的形状已在代码的注释中阐明："]},{"cell_type":"code","metadata":{"id":"nbfGXEsdfR8Q","colab_type":"code","colab":{}},"source":["class Encoder(tf.keras.Model):\n","  def __init__(self,vocab_size, embedding_dim,enc_units,batch_sz):\n","    super(Encoder,self).__init__()\n","    self.batch_sz = batch_sz\n","    self.enc_units = enc_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    \n","  def call(self, x, hidden):\n","    x = self.embedding(x)\n","    output,state = self.gru(x,initial_state = hidden)\n","    return output, state\n","\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_sz,self.enc_units))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMpitYNMgzM4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":79},"outputId":"c78dd032-eeb4-4c76-cdc0-4a2d846da82b","executionInfo":{"status":"ok","timestamp":1580623734112,"user_tz":-480,"elapsed":12477,"user":{"displayName":"方明旺","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnQeY1Sj3TbOfbHRabw-tZJn02nI0_dgYYjVYb=s64","userId":"06059548827349409826"}}},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","\n","# 样本输入\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n","Encoder Hidden state shape: (batch size, units) (64, 1024)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"swDyj3h8g_Qf","colab_type":"text"},"source":["#未完成，待续"]},{"cell_type":"code","metadata":{"id":"k7VmMWaaiXMh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}